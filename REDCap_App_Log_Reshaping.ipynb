{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# REDCap App Log/Project Log Reshaping\n",
        "This notebook reshapes the REDCap Mobile App Project Logs into Import Template format. It applies to situations when a project has multiple Data Collectors (RAs) in the field. For a number of them, both the \"Send Data to Server\" and \"Send Emergency Data Dump\" fail, but \"Send Project Log\" succeeds. This file parses the logs, reshapes them into the format of the import template, and combines them.\n",
        "\n",
        "The code is designed for multi-wave projects. Feel free to adapt it for single-wave projects."
      ],
      "metadata": {
        "id": "pWdaND4Psngl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "6px4gboWttNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3kZbuBOsk_K"
      },
      "outputs": [],
      "source": [
        "from os.path import exists\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract data from app log"
      ],
      "metadata": {
        "id": "77mbOnHzHXX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parser(df, n, template, RA, erroneous_column):\n",
        "  '''\n",
        "  There are three type of records in the App Log. \n",
        "  One type has both the \"onload_val\" and the \"modified_val\" variables. \n",
        "  The second type has only the \"modified_val\" variable. \n",
        "  The third type has neither.\n",
        "  This parser can parse the first two types, and it is called by the extractor function below.\n",
        "  When reading the import template, an erroneous column is usually introduced\n",
        "  ex. parser(df_change, 1, template, RA, erroneous_column)\n",
        "  '''\n",
        "  # parse the 'Data' column by the field\n",
        "  df_parsed = df.Data.str.split(r'\\\"\\|\\\"',expand=True)\n",
        "  # catch exception\n",
        "  if df_parsed.isnull().values.any():\n",
        "    print(\"There was an error in the parsing.\")\n",
        "    return None\n",
        "  else:\n",
        "    # add parsed data back\n",
        "    df = pd.concat([df, df_parsed], axis=1)\n",
        "    # extract record id\n",
        "    df_record_id = df[1].str.split(r'\\\"\\:\\\"',expand=True).rename(columns = {0:'record_id_column_name', 1:'record_id'})\n",
        "    # add record id back\n",
        "    df = pd.concat([df, df_record_id], axis=1)\n",
        "    # extract wave number\n",
        "    df_event_name = df[2].str.split(r'\\\"\\:\\\"',expand=True).rename(columns = {0:'event_name_column_name', 1:'redcap_event_name'})\n",
        "    # add wave number back\n",
        "    df = pd.concat([df, df_event_name], axis=1)\n",
        "    # extract field name\n",
        "    df_field_name = df[4].str.split(r'\\\"\\:\\\"',expand=True).rename(columns = {0:'field_name_column_name', 1:'field_name'})\n",
        "    # add field name back\n",
        "    df = pd.concat([df, df_field_name], axis=1)\n",
        "    # n==1 means the rows have both \"onload_val\" and \"modified_val\" variables\n",
        "    if n==1:\n",
        "      # extract modified value from df_change\n",
        "      df_modified_val = df[6].str.split(r'\\\"\\:\\\"',expand=True).rename(columns = {0:'modified_val_col_name', 1:'modified_val'})\n",
        "    # n==2 means the rows have only \"modified_val\" variable\n",
        "    elif n==2:\n",
        "      # extract modified value from df_no_change\n",
        "      df_modified_val = df[5].str.split(r'\\\"\\:\\\"',expand=True).rename(columns = {0:'modified_val_col_name', 1:'modified_val'})\n",
        "    # add modified value back\n",
        "    df = pd.concat([df, df_modified_val], axis=1)\n",
        "    # keep the latest change for a field\n",
        "    df = df.sort_values('Time').drop_duplicates(['record_id','redcap_event_name','field_name'],keep='last').reset_index(drop=True)\n",
        "    # join two field - prepare to pivot\n",
        "    cols=['record_id', 'redcap_event_name']\n",
        "    df['record_wave']=df[cols].apply(lambda row: ':'.join(row.values.astype(str)), axis=1)\n",
        "    # pivot\n",
        "    df = df.pivot(index='record_wave', columns='field_name', values='modified_val')\n",
        "    # remove the name of the column labels\n",
        "    df.columns.name=None\n",
        "    # reset index\n",
        "    df=df.reset_index()\n",
        "    # restore \"record_id\" and \"redcap_event_name\" columns\n",
        "    df_record_event = df['record_wave'].str.split(r\":\", expand=True).rename(columns = {0:'record_id', 1:'redcap_event_name'})\n",
        "    # add them back\n",
        "    df = pd.concat([df, df_record_event], axis=1)\n",
        "    # drop irrelevant columns\n",
        "    df.drop('record_wave', inplace=True, axis=1)\n",
        "    # drop rows with erroneous data\n",
        "    df=df[df['redcap_event_name']=='wave2_arm_1'].reset_index(drop=True)\n",
        "    # column names from df\n",
        "    ls=df.columns.to_list()\n",
        "\n",
        "    # read the import template\n",
        "    df_template=pd.read_csv(template)\n",
        "    # drop the erroneous column\n",
        "    df_template.drop(erroneous_column, inplace=True, axis=1)\n",
        "    # column names from template\n",
        "    ls_template=df_template.columns.to_list()\n",
        "\n",
        "    # find the columns that are in the data but not the template\n",
        "    ls_diff = list(set(ls).difference(ls_template))\n",
        "\n",
        "    # create the dataframe of columns to check by hand\n",
        "    ls_manual = ls_diff.copy()\n",
        "    ls_manual.append('record_id')\n",
        "    ls_manual.append('redcap_event_name')\n",
        "    df_manual=df[ls_manual]\n",
        "    df_manual.to_csv(RA + '_' + str(n) + '_manual.csv', index=False)\n",
        "\n",
        "    # merge the data into the template\n",
        "    df_template=pd.concat([df_template, df], ignore_index=True, sort=False)\n",
        "    # drop the extra columns\n",
        "    df_template.drop(ls_diff, inplace=True, axis=1)\n",
        "    df_template.to_csv(RA + '_' + str(n) + '_auto.csv', index=False)"
      ],
      "metadata": {
        "id": "WgZckvqwG00D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_from_log(log, template, RA, erroneous_column):\n",
        "  '''\n",
        "  This function extracts data from log and saves the data in the format of an Import Template\n",
        "  ex. extract_from_log('1668032732868.csv', 'ImportTemplate.csv', 'John', 'Unnamed: 1416')\n",
        "  '''\n",
        "  df_log = pd.read_csv(log)\n",
        "  # keep the rows that have the action \"MODIFY\"\n",
        "  df_log = df_log[df_log['Action']=='MODIFY'].reset_index(drop=True)\n",
        "  # these records have the onload_val column\n",
        "  df_change = df_log[df_log['Data'].str.contains(\"onload_val\")].reset_index(drop=True)\n",
        "  # these records do not have the onload_val column but have the modified_val column\n",
        "  df_no_change = df_log[~df_log['Data'].str.contains(\"onload_val\")].reset_index(drop=True)\n",
        "  df_no_change = df_no_change[df_no_change['Data'].str.contains(\"modified_val\")].reset_index(drop=True)\n",
        "  # these records do not have the modified_val column\n",
        "  df_no_modify = df_log[~df_log['Data'].str.contains(\"modified_val\")].reset_index(drop=True)\n",
        "  # parse the data\n",
        "  parser(df_change, 1, template, RA)\n",
        "  parser(df_no_change, 2, template, RA)\n",
        "  if not df_no_modify.empty:\n",
        "    df_parsed = df_no_modify.Data.str.split(r'\\\"\\|\\\"',expand=True)\n",
        "    # extract record id\n",
        "    df_record_id = df_parsed[1].str.split(r'\\\"\\:\\\"',expand=True).rename(columns = {0:'record_id_column_name', 1:'record_id'})\n",
        "    df_no_modify.to_csv(RA + '_' + str(3) + '_manual.csv')\n",
        "    df_record_id.to_csv(RA + '_' + str(3) + '_record_id.csv')"
      ],
      "metadata": {
        "id": "cKjcKbt7ikrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare record ids\n",
        "This section documents the records that are changed in the app log and compares them with the record ids that are given by the Project Manager (collected from the RAs)"
      ],
      "metadata": {
        "id": "a-H_Uzarwmnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A list of RAs\n",
        "ls_names = ['Amy', 'Blake', 'Catherine', 'Dan', 'Elise', 'Frank', 'George']"
      ],
      "metadata": {
        "id": "VLx1SKH3zg6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_record_ids(type, ls_names):\n",
        "  '''\n",
        "  ex. extract_record_ids('1_auto', ls_names)\n",
        "  '''\n",
        "  for i, name in enumerate(ls_names):\n",
        "    path = name + '_' + type + '.csv'\n",
        "    if exists(path):\n",
        "      df = pd.read_csv(name + '_' + type + '.csv')\n",
        "      if i==0:\n",
        "        ls_record_ids = df['record_id'].to_list()\n",
        "      else:\n",
        "        ls_record_ids = ls_record_ids + df['record_id'].to_list()\n",
        "  return ls_record_ids"
      ],
      "metadata": {
        "id": "Fluksxr72z-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls_1_auto = extract_record_ids('1_auto', ls_names)\n",
        "ls_1_manual = extract_record_ids('1_manual', ls_names)\n",
        "ls_2_auto = extract_record_ids('2_auto', ls_names)\n",
        "ls_2_manual = extract_record_ids('2_manual', ls_names)\n",
        "ls_3_record_id = extract_record_ids('3_record_id', ls_names)"
      ],
      "metadata": {
        "id": "U1F9MEKR3Tpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls_record_ids = ls_1_auto + ls_1_manual + ls_2_auto + ls_2_manual + ls_3_record_id"
      ],
      "metadata": {
        "id": "7x3QgZhN3nGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicates\n",
        "ls_record_ids = [*set(ls_record_ids)]"
      ],
      "metadata": {
        "id": "RpbrhSJz36cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ls_record_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq3otrX_1if0",
        "outputId": "2d4d8759-e904-4ffd-acf4-eefad0e6820c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import record ids obtained from the Project Manager\n",
        "df_2 = pd.read_csv('record_ids.csv')\n",
        "df_2"
      ],
      "metadata": {
        "id": "8AYgcrhr1CLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls_record_ids_2 = df_2['record_id'].to_list()\n",
        "ls_record_ids_2"
      ],
      "metadata": {
        "id": "7oRBogAr2LiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# records that are in the Project Manager's list but not yours\n",
        "ls_not_in = list(set(ls_record_ids_2) - set(ls_record_ids))\n",
        "ls_not_in.sort()\n",
        "ls_not_in"
      ],
      "metadata": {
        "id": "gaonAanB2RxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# records that are in your list but not the Project Manager's\n",
        "ls_should_not_be_in = list(set(ls_record_ids) - set(ls_record_ids_2))\n",
        "ls_should_not_be_in.sort()\n",
        "ls_should_not_be_in"
      ],
      "metadata": {
        "id": "QxjEuI2Vx7IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ls_should_not_be_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HlZfNQxzamT",
        "outputId": "5ce6e7ed-4d5b-4857-fa2f-53a75656d3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine records"
      ],
      "metadata": {
        "id": "GCZ_Z2XC0n47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_records(type, ls_name):\n",
        "  '''\n",
        "  Combine records between RAs\n",
        "  ex. combine_records('1_auto')\n",
        "  '''\n",
        "  for i, name in enumerate(ls_names):\n",
        "    path = name + '_' + type + '.csv'\n",
        "    if exists(path):\n",
        "      if i == 0:\n",
        "        df = pd.read_csv(path)\n",
        "      else:\n",
        "        df_temp = pd.read_csv(path)\n",
        "        df = pd.concat([df, df_temp]).reset_index(drop=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "d6rkbezn2URV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the code\n",
        "This subsection runs the code that combines the records"
      ],
      "metadata": {
        "id": "T1jGUoX2HknU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1_auto = combine_records('1_auto', ls_names)\n",
        "df_2_auto = combine_records('2_auto', ls_names)\n",
        "df_1_manual = combine_records('1_manual', ls_names)\n",
        "df_2_manual = combine_records('2_manual', ls_names)\n",
        "df_3_manual = combine_records('3_manual', ls_names)"
      ],
      "metadata": {
        "id": "zw7wxfAe3D1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1_auto.to_csv('1_auto.csv',index=False)\n",
        "df_2_auto.to_csv('2_auto.csv',index=False)\n",
        "df_1_manual.to_csv('1_manual.csv',index=False)\n",
        "df_2_manual.to_csv('2_manual.csv',index=False)\n",
        "df_3_manual.to_csv('3_manual.csv',index=False)\n",
        "# 1_auto.csv and 2_auto.csv are both for uploading to the project.\n",
        "# 1_manual.csv and 2_manual.csv can contain calculated fields and/or hidden fields whose values have changed\n",
        "# 3_manual.csv contains calculated fields and/or hidden fields with no value"
      ],
      "metadata": {
        "id": "v2TDQSo136N0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}